	The input dataset from Kaggle was converted into the Svmlight format, which included obtaining the ``best features'' that the Naïve Bayes algorithm produced as the ``important bag of words'', and creating the svmlight format from those particular word counts. We used Professor Joachims’s svmlight module executable to learn classifying models for each of the 24 labels, with varying c-values (to indicate the ``slack'' allowed for the classifier). 

	For each label, the best c-values (that produced the highest accuracy on the validation set) were used to prevent over-fitting in the final classifiers. These classifiers were inherently binary, and gave the geometric margin of a validation tweet from the separator. This geometric margin (normalized) was used to determine best the sentiment and when labels (by taking the max), and the best kind labels (those that were $\ge 0.7$).