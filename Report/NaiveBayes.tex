	We use Naive Bayes to learn the ``sentiment'' and ``when'' labels of a tweet, and to learn which kinds of weather are occuring. In addition to implementing the off-the-shelf version of the algorithm learned in class, we also took steps to make Naive Bayes a little less naive.
	
	Traditionally, in this setting, Naive Bayes breaks a tweet up into words and maintains probabilities corresponding to each word. Our implementation of the algorithm allows support for maintaining probabilities corresponding to groups of $k$ words for any $k$. For instance, if the tweet is ``not sunny today :('', we add the pairs of words ``not sunny'', ``sunny today'', and ``today :('' to our vocabulary (i.e. we look at bigrams). The groups of words consist of consecutive words as in this example. We experimented with different blends of unigrams, bigrams, and trigrams when performing Naive Bayes.
