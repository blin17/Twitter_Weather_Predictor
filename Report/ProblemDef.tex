\subsection{Task Definition}
	The problem definition was derived from a Kaggle competition, which we slightly modified to focus on the aspects of the problem that we found most interesting. The formal definition of our learning task is to classify a tweets “sentiment” regarding the weather, the time frame a tweet is referring to (called “when” of a tweet) and the “kind” of weather the user is tweeting about. These three main categories each have their own labels, which are stated below:     
 
\begin{itemize}
\item Sentiment = \{Can’t tell, Positive, Neutral, Negative, Unrelated to weather\}
\item When = \{Current, Future, Can’t tell, Past\}
\item Kind = \{Clouds, Cold, Dry, Hot, Humid, Hurricane, Can’t tell, Ice, Other, Rain, Snow, Storms, Sun, Tornado, Wind\}
\end{itemize}
 
	A tweet is given one label for the “sentiment” and “when” categories, however a tweet can be given multiple labels for the “kind” category. For example, the kind category of a tweet can be given the labels dry, hot, and humid.

	In the original problem the classification labels were given as confidence scores. However, we decided that it would be better to work with binary attributes. Thus we translated all the confidence scores into either zero or one. To do this we decided that if an attributes confidence scores is greater than or equal to 0.7 then it is labeled as “positive”, and if it is less than 0.3 then it is labeled as “negative.” By doing this we were better able to measure the performance of our learning algorithms.

	The data set given by Kaggle contained approximately 90,000 tweets. We used three quarters of this data for training and one quarter of it for testing. However, for building the Markov Random Field classifier we used a different data set. Since the MRF algorithm makes use of location and timestamp information to introduce conditional – dependencies to the Naïve Bayes algorithm, we had to look for a dataset that gave us that information. We were able to find such a training set in the Cheng-Caverlee-Lee dataset provided to us by Igor [1]. This dataset also included timestamp and locations along with the tweets, though without the classifications for each of the 24 labels.

\subsection{Algorithm}
	Given our input training sets (Kaggle and Cheng-Caverlee-Lee) and constraints, we decided to use a set of algorithms that we felt would best classify our data:

\begin{enumerate}
\item Support Vector Machine

We converted our input dataset to the Svmlight format and used Professor Joachims’s Svmlight module to learn and classify each of the 24 separate labels as binary classifiers. We then took the max of the resulting margins within the categories to single out the “best” labels.

\item Decision Tree

We used the TDIDT ID3 algorithm to build Decision Tree classifiers for each of the 24 labels, with splitting criteria $\ge 1$ and $\ge 0$. We then took the max of the resulting classifications to similarly decide on the “best” labels.

\item Naïve Bayes

We used independent class conditional probabilities of each significant word (or combination of words) in the tweet, and used that to calculate the probability of a label being assigned to a tweet.

\item Markov Random Field

	As an effort to make Naive Bayes less ``naive'', we introduced conditional dependencies (based on the location and timestamp of a tweet) to classify incoming test tweets better. This was based on the assumption that tweets in nearby locations and within the same intervals of time, would have some level of similarity when it comes to the kind of weather they would be talking about. Therefore, our MRF classifier made the use of these dependencies to classify incoming tweets based on previous tweets, weighing ``similar'' tweets greater than ``dissimilar'' ones when making a classification.
\end{enumerate}

	Each of these algorithms, along with the constraints we put on them, are explained in more detail in further sections.

